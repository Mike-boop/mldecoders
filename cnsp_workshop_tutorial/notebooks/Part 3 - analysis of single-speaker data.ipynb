{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b9a938",
   "metadata": {},
   "source": [
    "# Notebook 3: comparison of linear and nonlinear decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4078e109",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy==1.7.3\n",
    "\n",
    "!git clone https://github.com/Mike-boop/mldecoders.git\n",
    "\n",
    "import os\n",
    "os.chdir('mldecoders')\n",
    "\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed8d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the predictions from the CNSP web server \n",
    "\n",
    "!pip install gdown\n",
    "!gdown --folder https://drive.google.com/drive/folders/1vnZ1XDAlIKQH21S8uPo66ZHXaYdXOExu\n",
    "\n",
    "predictions_dir = \"Thornton_tutorial/\"\n",
    "\n",
    "# blue peter style predictions in case things go wrong:\n",
    "# predictions_dir = 'cnsp_workshop_tutorial/data/single-speaker-predictions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4450279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f148f73",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "In part 1, you trained linear decoders to predict the speech envelope from EEG recordings. In part 2, each of you trained one of the DNNs for one participant's data, and sent us the predicted speech envelope values. In this notebook, we will compare the performances of the linear models with those of the DNNs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8130e06",
   "metadata": {},
   "source": [
    "# Loading the predicted speech envelopes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88f2210",
   "metadata": {},
   "source": [
    "The outputs of the linear models which we saved can be loaded like so (using the first participant as an example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b9c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant = 0\n",
    "ridge_filepath = os.path.join(predictions_dir, f\"ridge_predictions_P{participant:02d}.npy\")\n",
    "ridge_predictions = np.load(ridge_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = np.load(os.path.join(predictions_dir, 'ground_truth.npy'))\n",
    "print(pearsonr(ridge_predictions, ground_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4710be",
   "metadata": {},
   "source": [
    "Similarly, the predictions of the DNNs can be loaded like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88554406",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcnn_filepath = os.path.join(predictions_dir, f\"fcnn_predictions_P{participant:02d}.npy\")\n",
    "fcnn_predictions = np.load(fcnn_filepath)\n",
    "print(pearsonr(fcnn_predictions, ground_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a10089",
   "metadata": {},
   "source": [
    "The time series can be compared visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3242b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_filepath = os.path.join(predictions_dir, f\"cnn_predictions_P{participant:02d}.npy\")\n",
    "cnn_predictions = np.load(cnn_filepath)\n",
    "\n",
    "fs = 125\n",
    "t = np.arange(len(cnn_predictions))/fs\n",
    "plt.plot(t, ground_truth, label='envelope')\n",
    "plt.plot(t, zscore(cnn_predictions), label='reconstruction')\n",
    "plt.legend()\n",
    "\n",
    "plt.xlim(10, 30)\n",
    "plt.xlabel('Time [s]')\n",
    "\n",
    "plt.title(f'correlation: {pearsonr(ground_truth, fcnn_predictions)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec704cd",
   "metadata": {},
   "source": [
    "## Exercise: correlate the CNN predictions with the FCNN predictions. What do you notice? Also compare the DNN predictions with the predictions of the linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af092f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c12f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae6141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81fd0d8b",
   "metadata": {},
   "source": [
    "# Population-level analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe50009",
   "metadata": {},
   "source": [
    "Let's collect the reconstruction accuracies (correlation coefficients) for each model and participant. We will also collect the null reconstruction scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeab96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = {'ridge': [], 'cnn': [], 'fcnn':[]}\n",
    "null_correlations = {'ridge': [], 'cnn': [], 'fcnn':[]}\n",
    "\n",
    "for participant in range(13):\n",
    "\n",
    "    for model in ['ridge', 'cnn', 'fcnn']:\n",
    "\n",
    "        filepath = os.path.join(predictions_dir, f\"{model}_predictions_P{participant:02d}.npy\")\n",
    "        predictions = np.load(filepath)\n",
    "\n",
    "        score = pearsonr(ground_truth, predictions)[0]\n",
    "        null_score = pearsonr(ground_truth[::-1], predictions)[0]\n",
    "\n",
    "        correlations[model].append(score)\n",
    "        null_correlations[model].append(null_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a867a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, tight_layout=True)\n",
    "\n",
    "axs[0].boxplot(correlations.values(), positions = [1,2,3])\n",
    "axs[0].set_ylabel('reconstruction score')\n",
    "axs[0].set_xticks([1,2,3])\n",
    "axs[0].set_xticklabels(correlations.keys())\n",
    "axs[0].set_ylim(-0.1, 0.4)\n",
    "\n",
    "axs[1].boxplot(null_correlations.values(), positions = [1,2,3])\n",
    "axs[1].set_ylabel('null reconstruction score');\n",
    "axs[1].set_xticks([1,2,3])\n",
    "axs[1].set_xticklabels(correlations.keys())\n",
    "axs[1].set_ylim(-0.1, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3799633e",
   "metadata": {},
   "source": [
    "Clearly, the reconstruction scores are much greater than the null reconstruction scores, so we can be confident that the speech envelope reconstruction is working better than chance. Interestingly, the null reconstruction scores seem to be overall slightly negative. __Exercise: can you think of why this might be? Is this a problem?__\n",
    "\n",
    "Additionally, the two DNNs appear to perform very similarly. We should perform a quick test to see whether they are significantly better than the linear models (note that really we should be performing multiple comparison corrections here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2010c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform single-tailed paired t-tests\n",
    "\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "print('CNN vs ridge: p =', ttest_rel(correlations['cnn'], correlations['ridge'], alternative='greater')[1])\n",
    "print('FCNN vs ridge: p =', ttest_rel(correlations['fcnn'], correlations['ridge'], alternative='greater')[1])\n",
    "\n",
    "# additionally, compare the DNNs with a two-tailed paired t-test\n",
    "\n",
    "print('FCNN vs CNN: p =', ttest_rel(correlations['fcnn'], correlations['cnn'], alternative='two-sided')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfb4917",
   "metadata": {},
   "source": [
    "# Extension: two-speaker data\n",
    "\n",
    "Having explored using DNNs to reconstruct the envelope of clean speech from EEG recordings, we could now look at more exciting listening conditions. In our paper, we considered competing-speakers conditions, speech-in-babble-noise conditions, and speech in an unfamiliar language.\n",
    "\n",
    "We provide the outputs of the DNNs when they were applied to competing-speakers conditions. There were two conditions:\n",
    "\n",
    "- __fM__: the participants focussed on the male speaker, whilst ignoring the female speaker\n",
    "- __fW__: the participants focussed on the female speaker, whilst ignoring the male speaker\n",
    "\n",
    "In this extension section, we invite you to explore this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b83f60",
   "metadata": {},
   "source": [
    "The data may loaded like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b761e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first consider the fM condition\n",
    "\n",
    "# get the speech envelope of the attended speaker\n",
    "fM_attended_envelope = np.load('cnsp_workshop_tutorial/data/two-speakers-predictions/fM_attended_truth.npy')\n",
    "\n",
    "# get the speech envelope of the unattended speaker\n",
    "fM_unattended_envelope = np.load('cnsp_workshop_tutorial/data/two-speakers-predictions/fM_unattended_truth.npy')\n",
    "\n",
    "# get the predicted speech envelope for the first participant\n",
    "# explore changing the participant, or using 'ridge' or 'fcnn' in place of 'cnn'\n",
    "participant = 1\n",
    "fpath = f'cnsp_workshop_tutorial/data/two-speakers-predictions/fM_predictions_YH{participant:02d}_cnn.npy'\n",
    "fM_predictions = np.load(fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5e5c7b",
   "metadata": {},
   "source": [
    "How well do the predictions correlate with the attended and unattended envelopes, for this participant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0199953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"attended: \", pearsonr(fM_predictions, fM_attended_envelope)[0])\n",
    "print(\"unattended: \", pearsonr(fM_predictions, fM_unattended_envelope)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e383f8",
   "metadata": {},
   "source": [
    "Notice that the reconstruction is more similar to the attended speech envelope than it is to the unattended speech envelope. This observation may be used for _auditory attention decoding_. We can determine an attention decoding accuracy, by dividing the data into (for example) 5-second windows, and comparing the correlations between the reconstruction and each of the attended and unattended speech envelopes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66a7b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.helpers import get_scores\n",
    "fs = 125 \n",
    "window = 5\n",
    "\n",
    "# divide the time series into 5s windows, and compute the correlation coefficients in each window\n",
    "ridge_attended_scores = get_scores(fM_attended_envelope, fM_predictions, batch_size=window*fs)\n",
    "ridge_unattended_scores = get_scores(fM_unattended_envelope, fM_predictions, batch_size=window*fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecb8e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the attended and unattended reconstruction scores\n",
    "plt.boxplot([ridge_attended_scores, ridge_unattended_scores]);\n",
    "plt.ylabel('reconstruction score')\n",
    "plt.xticks([1,2], ['attended', 'unattended'])\n",
    "\n",
    "# calculate the proportion of windows for which the attended reconstruction score is greater than the unattended reconstruction score\n",
    "# this proportion is the attention decoding accuracy\n",
    "num_windows = len(ridge_attended_scores)\n",
    "num_correctly_decoded = np.sum(ridge_attended_scores > ridge_unattended_scores)\n",
    "decoding_accuracy = (num_correctly_decoded/num_windows * 100)\n",
    "\n",
    "plt.title(\"decoding accuracy: \" + str(round(decoding_accuracy, 2)) + \"%\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465081aa",
   "metadata": {},
   "source": [
    "## Over to you!\n",
    "\n",
    "Suggestions:\n",
    "\n",
    "- is the reconstruction of the unattended speech stream above chance level?\n",
    "- how does the attention decoding accuracy vary between participants?\n",
    "- how does the attention decoding accuracy vary with window size?\n",
    "- is the pearson correlation coefficient the best metric for assessing the reconstruction accuracy/decoding auditory attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bdb421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
