{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be7c3394",
   "metadata": {},
   "source": [
    "# Notebook 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd16c55c",
   "metadata": {},
   "source": [
    "Please enable hardware acceleration for this notebook. Go to the 'runtime' tab, select 'change runtime type', and select 'GPU' as the hardware accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226b8407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print('Please enable GPU hardware acceleration!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6c42d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy==1.7.3\n",
    "!pip install optuna==2.10.0\n",
    "\n",
    "!git clone https://github.com/Mike-boop/mldecoders.git\n",
    "\n",
    "import os\n",
    "os.chdir('mldecoders')\n",
    "\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eede7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data from the CNSP web server \n",
    "# This may take several minutes\n",
    "\n",
    "!curl https://www.data.cnspworkshop.net/data/thornton_data/data.h5 --output data.h5\n",
    "data_dir = 'data.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45707946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import optuna\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from torch.utils.data import DataLoader\n",
    "from pipeline.dnn import FCNN, CNN\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8582ed17",
   "metadata": {},
   "source": [
    "# Deep neural networks\n",
    "\n",
    "Unlike the linear models of the last notebook, deep neural networks are heavily parameterised, nonlinear models. The neural networks here consist of stacked layers of interconnected nodes. The FCNN has around 12 million parameters, whereas the CNN has around 9000 parameters. For comparison, the linear models have around 3000 parameters.\n",
    "\n",
    "Unlike the linear models, the DNNs have no closed-form solution for the optimal coefficients. Therefore, we resort to gradient-descent-based algorithms to tune their parameters.\n",
    "\n",
    "The DNNs rely on a number of hyperparameters (whereas the linear models only relied on the regularisation parameter). In this tutorial, to save time, we have provided a set of hyperparameters already tuned to the envelope decoding task. We will show you how to tune one of the hyperparameters called the learning rate.\n",
    "\n",
    "<img src=\"../images/architectures.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbec1f4",
   "metadata": {},
   "source": [
    "# Fitting the FCNN for a single participant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7803adbe",
   "metadata": {},
   "source": [
    "We will use the same training/validation/testing data split as we did in the paper: nine trials will be used for training, three for validating, and three for testing. Let's look at fitting the FCNN to data from the first participant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5c052",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant = 0\n",
    "\n",
    "train_parts = range(9)\n",
    "val_parts = range(9,12)\n",
    "test_parts = range(12, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54abcaea",
   "metadata": {},
   "source": [
    "Now let's define the pre-determined hyperparameters. We will overwrite the learning (`lr`) parameter later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f4e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcnn_hyperparameters = {\"num_hidden\": 3, \"dropout_rate\": 0.45}\n",
    "fcnn_train_params = {\"lr\": 0.0001, \"batch_size\": 256, \"weight_decay\": 0.0001}\n",
    "\n",
    "cnn_hyperparameters = {\"dropout_rate\": 0.20,\"F1\": 8,\"D\": 8,\"F2\": 64}\n",
    "cnn_train_params = {\"lr\": 0.01, \"batch_size\": 256, \"weight_decay\": 1e-08}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b035e15",
   "metadata": {},
   "source": [
    "To train the FCNN, we have to iterate over the training dataset in batches, performing gradient descent steps as we go. Each complete iteration through the training dataset is termed an 'epoch'. After each epoch, we evaluate how good the fit is by calculating the reconstruction score (pearson correlation) on the validation dataset.\n",
    "\n",
    "This optimisation procedure is contained within the `train_dnn` function.\n",
    "\n",
    "The `train_dnn` function requires several arguments:\n",
    "\n",
    "- `data_dir`: the location of the data file\n",
    "- `participants`: an integer or list of integers, specifying the participants whose data will be used to train the DNN\n",
    "- `train_parts` and `val_parts`: the training/validation trials used\n",
    "- `epochs`: the number of times we iterate over the training dataset\n",
    "- `early_stopping_patience`: a stopping criterion if the validation score stops increasing before the last `epoch`.\n",
    "\n",
    "Additionally, we need to supply the hyperparameters that we want to use. These have been separated into `train_params` and `hyperparameters`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743742bf",
   "metadata": {},
   "source": [
    "Try out the `train_dnn` function below. The printed numbers indicate the validation scores after each epoch.\n",
    "\n",
    "__Question__: how many times was the validation score evaluated? Is this what you expected based on the values of `epochs` and `early_stopping_patience`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f96a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will take around 5 minutes to run\n",
    "\n",
    "from pipeline.training_functions import train_dnn\n",
    "\n",
    "validation_accuracy, state_dict = train_dnn(data_dir,\n",
    "                                            participants = participant,\n",
    "                                            checkpoint_path=None,\n",
    "                                            model_handle=FCNN,\n",
    "                                            train_parts = train_parts,\n",
    "                                            val_parts = val_parts,\n",
    "                                            epochs=20,\n",
    "                                            early_stopping_patience=3,\n",
    "                                            workers=2,\n",
    "                                            **fcnn_train_params,\n",
    "                                            **fcnn_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3045c867",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c0f03e",
   "metadata": {},
   "source": [
    "The `train_dnn` function has two outputs:\n",
    "\n",
    "- `validation_accuracy`: this is the best validation score achieved during training\n",
    "- `state_dict`: this is a dictionary which contains the parameters of the best model\n",
    "\n",
    "We can load the best parameters into the FCNN for evaluation on the testing dataset, using the `state_dict` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18c38d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = FCNN(**fcnn_hyperparameters)\n",
    "trained_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7690ad5f",
   "metadata": {},
   "source": [
    "In order to evaluate the DNN, we can use the `get_dnn_predictions` function. This function requires two inputs:\n",
    "\n",
    "- `trained_model`: a DNN object, loaded with the optimal parameters.\n",
    "- `test_loader`: a DataLoader object for the test dataset.\n",
    "\n",
    "In PyTorch, `DataLoader` objects are convenient objects for representing datasets. We can make one in two lines as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.evaluation_functions import get_dnn_predictions\n",
    "from pipeline.datasets import HugoMapped\n",
    "\n",
    "test_dataset = HugoMapped(test_parts, data_dir, participant)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=512)\n",
    "\n",
    "prediction = get_dnn_predictions(trained_model, test_loader, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072c43c2",
   "metadata": {},
   "source": [
    "Now we can correlate the FCNN predictions against the actual speech envelope. Note that we didn't zero-pad the test dataset, so the envelope and our prediction of the envelope will have different sizes (the two will differ by the duration of the spatiotemporal input window). We correct for this by ignoring the last few values of the true speech envelope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f91112",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(data_dir, 'r') as f:\n",
    "    envelope = np.hstack([f[f'stim/part{j}'][:] for j in test_parts])\n",
    "    \n",
    "print(pearsonr(prediction, envelope[:-50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd8dc5",
   "metadata": {},
   "source": [
    "## Exercise: compute the null reconstruction score of the FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d422d042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d7525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24399f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8eaf345",
   "metadata": {},
   "source": [
    "# Tuning the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0706591",
   "metadata": {},
   "source": [
    "When we fitted the linear models, we found that the ridge regression regularisation parameter had a big effect on the validation score. Since the DNNs rely on a larger set of hyperparameters, it can take quite a lot of time to tune these.\n",
    "\n",
    "To reduce the amount of time required to train these models, we can assume that most of the hyperparameters are the same for all of the participants, and identify them by training a population model using data from all participants. In fact, the provided parameters were determined in this way.\n",
    "\n",
    "In order to improve performance, and to demonstrate how to tune the DNN hyperparameters, we will re-tune the learning rate parameter for one of the participants. We'll use less epochs here to reduce runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d682e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first delete the original learning rate estimate\n",
    "del fcnn_train_params['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0301c393",
   "metadata": {},
   "source": [
    "To tune the learning rate, we need to create an objective function to maximise with respect to the learning rate. Our objective function will return the best validation accuracy as returned by `train_dnn`. We also define a set of candidate learning rates to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92b0d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcnn_objective(trial):\n",
    "\n",
    "    lr =  trial.suggest_loguniform('lr', 1e-8, 1e-1)\n",
    "    print('>',lr)\n",
    "\n",
    "    validation_accuracy, state_dict = train_dnn(data_dir,\n",
    "                                                participants = participant,\n",
    "                                                checkpoint_path=None,\n",
    "                                                model_handle=FCNN,\n",
    "                                                train_parts = train_parts,\n",
    "                                                val_parts = val_parts,\n",
    "                                                epochs=5,\n",
    "                                                early_stopping_patience=3,\n",
    "                                                workers=2,\n",
    "                                                **fcnn_train_params,\n",
    "                                                **fcnn_hyperparameters,\n",
    "                                                lr=lr)\n",
    "    \n",
    "    trial.set_user_attr(key=\"best_state_dict\", value=state_dict)\n",
    "    return validation_accuracy\n",
    "\n",
    "gridsampler = optuna.samplers.GridSampler({\"lr\": [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d312aff8",
   "metadata": {},
   "source": [
    "In order to extract the best model parameters at the end, we need to create a 'callback' function. This allows us to save user-defined variables during hyperparameter tuning. We will save the `state_dict` of the best model, which contains the optimal model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a65a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_mdl_callback(study, trial):\n",
    "    if study.best_trial.number == trial.number:\n",
    "        study.set_user_attr(key=\"best_state_dict\", value=trial.user_attrs[\"best_state_dict\"])\n",
    "\n",
    "# Median pruning is an alternative stopping criterion to early stopping.\n",
    "# This line turns off median pruning.\n",
    "fcnn_pruner = optuna.pruners.MedianPruner(n_startup_trials=np.infty)\n",
    "\n",
    "fcnn_study = optuna.create_study(\n",
    "   direction=\"maximize\",\n",
    "   sampler=gridsampler,\n",
    "   pruner=fcnn_pruner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496a8a5e",
   "metadata": {},
   "source": [
    "Now we can optimize the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f64104",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fcnn_study.optimize(fcnn_objective, n_trials=5, callbacks=[trained_mdl_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7823290",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789296d5",
   "metadata": {},
   "source": [
    "Now we can extract the best parameters, and as before we can load them into the FCNN and calculate the reconstruction score (Pearson correlation coefficient) on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = fcnn_study.user_attrs['best_state_dict']\n",
    "\n",
    "trained_model = FCNN(**fcnn_hyperparameters)\n",
    "trained_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c2d0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcnn_prediction = get_dnn_predictions(trained_model, test_loader, device='cuda')\n",
    "pearsonr(fcnn_prediction, envelope[:-50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd807611",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a02ac",
   "metadata": {},
   "source": [
    "# 1. How does the ridge regression compare against the FCNN for the first participant?\n",
    "\n",
    "- hint: by splitting the data into windows, and calculating the correlation coefficients in each batch, we can build up a distribution of reconstruction scores. We can use a t-test to test for differences between the ridge distribution and the FCNN distribution.\n",
    "- hint: the `get_scores` function will compute the correlations in windows of a given size, illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8601cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.helpers import get_scores\n",
    "\n",
    "with h5py.File(data_dir, 'r') as f:\n",
    "    envelope = np.hstack([f[f'stim/part{j}'][:] for j in test_parts])\n",
    "    \n",
    "ridge_predictions = np.load(\"cnsp_workshop_tutorial/results/linear_models/P00_predictions.npy\")[0]\n",
    "\n",
    "# get correlation coefficient in each 5-second window:\n",
    "ridge_scores = get_scores(envelope, ridge_predictions, batch_size=5*125)\n",
    "\n",
    "plt.hist(ridge_scores, bins=10)\n",
    "\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Reconstruction score\")\n",
    "plt.title(\"Ridge correlations in 5-second windows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4405210",
   "metadata": {},
   "source": [
    "# 2. Tune the learning for your assigned DNN and participant. Save your predictions on the test dataset, and email them to me at mdt20@ic.ac.uk\n",
    "\n",
    "- We have tuned the learning rate for the FCNN, for participant 0.\n",
    "\n",
    "- We would like to have predicted speech envelopes for both of the DNNs, and for each of the participants.\n",
    "\n",
    "- You will be assigned a DNN (either CNN or FCNN) and a participant (between 1 and 13 inclusive). The code for fitting the DNNs was demonstrated in this notebook. It is reproduced below for convenience. Please modify the `participant` and `model_handle` fields appropriately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e678be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the objective function\n",
    "# comments indicate lines you may need to change\n",
    "\n",
    "participant = 0 # a number between 1 and 13\n",
    "model_handle = FCNN # CNN\n",
    "train_params = fcnn_train_params #cnn_train_params\n",
    "hyperparameters = fcnn_hyperparameters #cnn_hyperparameters\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    lr =  trial.suggest_loguniform('lr', 1e-8, 1e-1)\n",
    "    print('>',lr)\n",
    "\n",
    "    validation_accuracy, state_dict = train_dnn(data_dir,\n",
    "                                                participants = participant,\n",
    "                                                checkpoint_path=None,\n",
    "                                                model_handle=model_handle,\n",
    "                                                train_parts = train_parts,\n",
    "                                                val_parts = val_parts,\n",
    "                                                epochs=5,\n",
    "                                                early_stopping_patience=3,\n",
    "                                                workers=2,\n",
    "                                                **train_params,\n",
    "                                                **hyperparameters,\n",
    "                                                lr=lr)\n",
    "    \n",
    "    trial.set_user_attr(key=\"best_state_dict\", value=state_dict)\n",
    "    return validation_accuracy\n",
    "\n",
    "gridsampler = optuna.samplers.GridSampler({\"lr\": [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ba87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizing the objective function\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=np.infty)\n",
    "\n",
    "study = optuna.create_study(\n",
    "   direction=\"maximize\",\n",
    "   sampler=gridsampler,\n",
    "   pruner=pruner\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=5, callbacks=[trained_mdl_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5327e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the best model\n",
    "\n",
    "state_dict = study.user_attrs['best_state_dict']\n",
    "\n",
    "trained_model = model_handle(**hyperparameters)\n",
    "trained_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ff144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting and saving the dnn predictions on the test data\n",
    "\n",
    "test_dataset = HugoMapped(test_parts, data_dir, participant)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=512)\n",
    "\n",
    "predictions = get_dnn_predictions(trained_model, test_loader, device='cuda')\n",
    "np.save(f\"{str(model_handle)}_predictions_P{participant:02d}.npy\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f1039a",
   "metadata": {},
   "source": [
    "Please download the resulting .npy file (from the file browser on the left) and email it to me."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
